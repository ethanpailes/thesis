\chapter{Compilation}
\label{chapter:compilation}

\section{Analysis}

As is often the case when it comes to compile time optimizations,
the optimizations presented in this thesis cannot always be
applied. In order to determine when they can be applied, we must
first perform various analyses. This section introduces concepts
that help us determine when it is safe to scan or jump forward over the
input corresponding to a particular subexpression.

\subsection{First Sets}

In order to make sure that the correct branch is taken in cases of
ambiguity, we will need to be able to talk about the set of characters
which might possibly start a regex match. To this end we introduce the
notion of a \emph{first set} here. Let $e$ be a regular expression.
The first set of an expression, $e$, is given by
$\{first(w) \rvert w \in L(e)\}$ where $first$ takes just the first
character from a word. The first set for a given
regular expression can be computed by the recursive function $fset$,
which is defined in figure \ref{fig:fsetdef}.

\begin{figure}
\label{fig:fsetdef}
\caption{fset}
\begin{tabular}{l c l c l c l}
$fset(e_1 e_2)$ & $=$ & $fset(e_1)$ & &
  $fset(e_1 \rvert e_2)$ & $=$ & $fset(e_1) \cup fset(e_2)$ \\
$fset((e))$ & $=$ & $fset(e)$ & &
  $fset(\epsilon)$ & $=$ & $\emptyset$ \\
$fset(\alpha)$ & $=$ & $\alpha$ & &
  $fset(e*)$ & $=$ & $fset(e)$ \\
$fset(e+)$ & $=$ & $fset(e)$ & &
  $fset(e*?)$ & $=$ & $fset(e+?)$ \\
$fset(.)$ & $=$ & $\Sigma$ & & & & \\
\end{tabular}
\end{figure}

\subsection{Regular Expression Intersection: A Decision Problem}
\label{section:regexinterdecide}

In this section we introduce the concept of regular expression
intersection, and specialize it to a decision problem determining
if the intersection of two regex is empty.
When no string is in the language of both regex, it is
safe to scan forward over one regex to reach the second.
In particular, the optimizations presented in section
\ref{section:littermscan} would not be possible without
the ability to decide if the intersection of two regex is empty.

\subsubsection{Intersection Automatons}

The easiest way to reason about regular expression intersection
is through automaton. To decide the intersection of two NFAs
(as defined in section \ref{section:nfadef}) we
can run them both in lockstep on the same input stream
and see if they both end up in accepting states at the same time.
Running two NFAs in lockstep requires a specific input, so this insight
does not allow us to do any static checking, but it sets us in the
right direction.

A regex automaton decides the empty set if no final state is reachable
from the start state, so if we can construct a new NFA which simulates
running two NFAs in lockstep, we will be able to statically determine
if the intersection of two regex is empty using standard graph
search algorithms. We construct an
intersecting automaton by taking the Cartesian product
of the two state sets to form a new state set, then adding edges
between states only where both NFAs could make progress on a particular
character of input. This idea is unpacked more formally below.

Let $e_1$ and $e_2$ be regular expressions. Let
$n_1 = \langle Q_1, \Sigma, \Delta_1, q_{1,0}, F_1 \rangle$ and
$n_2 = \langle Q_2, \Sigma, \Delta_2, q_{2,0}, F_2 \rangle$
be NFAs such that $n_1$ decides $e_1$ and $n_2$ decides $e_2$.
Construct $n_i = \langle Q_i, \Sigma, \Delta_i, \langle q_{1,0}, q_{2,0} \rangle, F_i \rangle$
where
$$Q_i = Q_1 \times Q_2$$
$$F_i = \{ \langle q_1, q_2 \rangle \mid q_1 \in F_1 \wedge q_2 \in F_2\}$$
\begin{equation}
\begin{split}
  \Delta_i =
    \{ \langle \langle q_{1, f}, q_{2, f} \rangle, c \rangle
            \rightarrow \langle q_{1, t}, q_{2, t} \rangle
        \mid
          (\langle q_{1, f}, c \rangle \rightarrow q_{1, t}) \in \Delta_1
          \wedge
          (\langle q_{2, f}, c \rangle \rightarrow q_{2, t}) \in \Delta_2
     \} \\
     \cup
     \{
        \langle \langle q_{1, f}, q_{2, f} \rangle, \epsilon \rangle
          \rightarrow \langle q_{1, f}, q_{2, t} \rangle
        \mid
          (\langle q_{1, f}, c \rangle \rightarrow q_{1, t}) \in \Delta_1
          \wedge
          (\langle q_{2, f}, \epsilon \rangle \rightarrow q_{2, t}) \in \Delta_2
     \} \\
     \cup
     \{
        \langle \langle q_{1, f}, q_{2, f} \rangle, \epsilon \rangle
          \rightarrow \langle q_{1, t}, q_{2, f} \rangle
        \mid
          (\langle q_{1, f}, \epsilon \rangle \rightarrow q_{1, t}) \in \Delta_1
          \wedge
          (\langle q_{2, f}, c \rangle \rightarrow q_{2, t}) \in \Delta_2
   \} \\
     \cup
     \{
        \langle \langle q_{1, f}, q_{2, f} \rangle, \epsilon \rangle
          \rightarrow \langle q_{1, t}, q_{2, t} \rangle
        \mid
          (\langle q_{1, f}, \epsilon \rangle \rightarrow q_{1, t}) \in \Delta_1
          \wedge
          (\langle q_{2, f}, \epsilon \rangle \rightarrow q_{2, t}) \in \Delta_2
       \}
\end{split}
\end{equation}

The most interesting part of this construction is the creation of the
new delta function. Edges are only added to $\Delta_i$ when a particular
character $c$ shows up in both input delta functions. The first set in
the union covers this most intuitive case, but we still need to consider
the presence of $\epsilon$ edges. The last three sets add edges in the
compound NFA when $\epsilon$ shows up in either of the input NFAs.
Note the way that only the side of the compound state with an
epsilon advances.

To illustrate this process consider the intersection of NFAs corresponding
to \verb'/aa/' (figure \ref{fig:NFAaa}) and \verb'/ab/'
(figure \ref{fig:NFAab}), shown in figure \ref{fig:NFAaa:ab}. Note that
we have omitted intermediary states with no edges for brevity, but the
full intersection NFA would have states $q_{0,1}, q_{1,2}$ and so on.
It is easy to see by inspection of the diagram that the intersection of
\verb'/aa/' and \verb'/ab/' is empty, as expected because
\verb'/aa/' does not accept any strings containing \verb'b', while
\verb'/b/' demands a \verb'b' in its input.

To see an example where the intersection is non-empty, consider the
intersection of \verb'/aa/' (figure \ref{fig:NFAaa}) and \verb'/a*/'
(figure \ref{fig:NFAastar}), shown in figure \ref{fig:NFAaa:astar}.
Again, we have omitted the orphan intermediary states for brevity.
The path $q_{0,0}, q_{0,1}, q_{1,2}, q_{1,1}, q_{2,3}$, flows from
the start state to the accepting state, so the intersection of the
two regex is not empty, as we would expect.

\begin{figure}
\centering
\caption{{\tt /aa/} NFA}
    \input{fig/aa-nfa}
\label{fig:NFAaa}
\end{figure}

\begin{figure}
\centering
\caption{{\tt /ab/} NFA}
    \input{fig/ab-nfa}
\label{fig:NFAab}
\end{figure}


\begin{figure}
\centering
\caption{{\tt /aa/} $\cap$ {\tt /ab/} NFA}
    \input{fig/aa-intersect-ab}
\label{fig:NFAaa:ab}
\end{figure}

\begin{figure}
\centering
\caption{{\tt /a*/} NFA}
    \input{fig/astar-nfa}
\label{fig:NFAastar}
\end{figure}

\begin{figure}
\centering
\caption{{\tt /aa/} $\cap$ {\tt /a*/} NFA}
    \input{fig/aa-intersect-astar}
\label{fig:NFAaa:astar}
\end{figure}

\subsubsection{Regex Intersection on VM Instructions}

While regular expression intersection is most elegantly stated
in terms of abstract NFAs and the construction of an entirely
new automata, this approach needs to be modified if it is
to be used in the context of a VM NFA simulation. A regex
VM program is a representation of an NFA, but not in the usual
set-of-states and transition
table sort of way. Additionally, constructing an entirely
new NFA consumes space on the order of the product of the
size of the two input NFAs. Instead, we use the concept of
iterators, a primary warcry of the Rust programming language,
to provide the ability to explore a compound NFA without
ever materializing it. Rather than using a compiled
skip regex we use a regex compiled to use the unextended
set of VM instructions. This simplifies our job by reducing
the number of instructions we need to handle, and prevents
compiling skip instructions from recursively compiling any
subexpressions, which improves compiler performance.

To see how to decide the intersection of two NFAs described
by VM instructions, consider algorithm \ref{algo:regexinter}.
In the algorithm presented we use iterator generators
(expressed with the \verb'yield' keyword),
a feature common to programming languages which support coroutines
such as Python. Helper iterators over a single NFA are defined
in algorithm \ref{algo:nfaiter}. Rust does not yet support generators, so the
actual implementation instead uses explicit state machines, but
we use iterator generators here to make it easier to focus on the underlying
algorithm.

\begin{algorithm}
\caption{VM NFA Intersection} \label{algo:regexinter}
\begin{algorithmic}
\Procedure{NFAIntersectionIsEmpty}{$n_1$, $n_2$}
  \State Assume the existence of a $start$ function defined on
          VM programs which returns the starting instruction.
  \State Assume the existence of a $next$ function defined on
          VM program instructions which returns the next instruction
          in the program.
  \State $resume \gets [\langle start(n_1), start(n_2) \rangle]$
  \State $seen \gets \{\}$
  \While{$\langle s_1, s_2 \rangle \gets resume.pop()$}
    \If{$\langle s_1, s_2 \rangle \in seen$}
      \State \textbf{continue}
    \EndIf
    \State $seen \gets seen \cup \{\langle s_1, s_2 \rangle\}$
    \If{$s_1 = {\tt match} \wedge s_2 = {\tt match}$}
      \State \textbf{return} true
    \EndIf
    \State $resume.extend(IntersectChildren(s_1, s_2))$
  \EndWhile
  \State \textbf{return} false
\EndProcedure
\Procedure{IntersectChildren}{$inst_1$, $inst_2$}
  \For{$c1 \in$ Children($inst_1$)}
    \For{$c2 \in$ Children($inst_2$)}
      \Switch{$c1$}
        \Case{{\tt match}}
          \State \textbf{yield} $\langle c1, c2 \rangle$
          \State \textbf{break}
        \EndCase
        \Case{{\tt char} $\alpha_1$}
          \Switch{$c2$}
            \Case{{\tt match}}
              \State \textbf{yield} $\langle c1, c2 \rangle$
              \State \textbf{break}
            \EndCase
            \Case{{\tt char} $\alpha_2$}
              \If{$\alpha_1 \cap \alpha_2 \not= \emptyset$}
                \State \textbf{yield} $\langle c1, c2 \rangle$
              \EndIf
              \State \textbf{break}
            \EndCase
            \Case{default}
              \State unreachable
            \EndCase
          \EndSwitch
          \State \textbf{break}
        \EndCase
        \Case{default}
          \State unreachable
        \EndCase
      \EndSwitch
    \EndFor
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{VM NFA Iteration} \label{algo:nfaiter}
\begin{algorithmic}
\Procedure{Children}{$inst$}
  \State clear seen markings
  \Switch{$inst$}
    \Case{{\tt split L1 L2}}
      \State Terminals({\tt L1})
      \State Terminals({\tt L2})
      \State \textbf{break}
    \EndCase
    \Case{{\tt jmp L}}
      \State Terminals({\tt L})
      \State \textbf{break}
    \EndCase
    \Case{{\tt match}} \EndCase
    \Case{{\tt char} $\alpha$} \EndCase
    \Case{{\tt save} $s$}
      \State Terminals($next(inst)$)
      \State \textbf{break}
    \EndCase
  \EndSwitch
\EndProcedure
\Procedure{Terminals}{$inst$}
  \If{$inst$ is marked as seen}
    \State \textbf{return}
  \EndIf
  \State mark $inst$ as seen
  \Switch{$inst$}
    \Case{{\tt split L1 L2}}
      \State Terminals({\tt L1})
      \State Terminals({\tt L2})
      \State \textbf{break}
    \EndCase
    \Case{{\tt jmp L}}
      \State Terminals({\tt L})
      \State \textbf{break}
    \EndCase
    \Case{{\tt match}} \EndCase
    \Case{{\tt char} $\alpha$}
      \State \textbf{yield} $inst$
      \State \textbf{break}
    \EndCase
    \Case{{\tt save} $s$}
      \State Terminals($next(inst)$)
      \State \textbf{break}
    \EndCase
  \EndSwitch
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Compiling without Optimization}
\label{section:compilation}

In this section, we define \verb'code', a compilation function mapping
from regular expressions to VM instructions. The approach taken here
draws heavy inspiration from Russ Cox's writeup of the virtual machine
approach to regular expressions (\cite{CoxVirtualMachineApproach}).
The \verb'code' function we present does not produce optimized code,
but serves as a baseline over which we will layer optimizations
in section \ref{section:skipscanopt}.
The \verb'code' function performs some simple case analysis on its
input and emits a recursively defined sequence of VM instructions when applied
to a regular expression.
The particulars of this translation can
be found in table \ref{table:codedef}.

In the context of VM instructions, sets are written in
\texttt{SCREAMING\allowbreak\_SNAKE\allowbreak\_CASE}.
When a new label is mentioned in table
\ref{table:codedef} assume that it is fresh and unique as if it has
come from a \verb'gensym' like routine.

\begin{table}[ht]
\label{table:codedef}
\caption{Code Function}

\centering
\begin{tabular}{| l | l | p{8cm} |} \hline
Expression & Emitted VM Code & Discussion \\ \hline
$e_1 e_2$ &
  \begin{minipage}{3cm}
  \begin{verbatim}

code(e1)
code(e2)
  \end{verbatim}
  \end{minipage}
  & \\ \hline
$e_1 \rvert e_2$ &
  \begin{minipage}{3cm}
  \begin{verbatim}

split L1 L2
L1: code(e1)
    jmp L3
L2: code(e2)
L3:
  \end{verbatim}
  \end{minipage}
  & \\ \hline
$(e)$ &
  \begin{minipage}{3cm}
  \begin{verbatim}

save n
code(e)
save n+1
  \end{verbatim}
  \end{minipage}
  & 
  Here \verb'n' is given by $(i*2) + 2$ where $i$ is the index of
  the capture group being compiled in the regular expression. Slots $0$
  and $1$ are reserved for capturing the entire match, which is
  where the extra $2$ in the formula comes from. A nice corollary
  is that we can always know the number of slots that
  will be required ahead of time by computing $(c*2) + 2$
  where $c$ is the total number of capture groups.
  \\ \hline
$\epsilon$ & & The empty code block.  \\ \hline
$\alpha$ & \verb'char ALPHA' & Here \verb'ALPHA' is $\alpha$ \\ \hline
$e*$ &
  \begin{minipage}{3cm}
  \begin{verbatim}

split L1 L2
L1: code(e)
    split L1 L2
L2: 
  \end{verbatim}
  \end{minipage}
  & \\ \hline
$e+$ &
  \begin{minipage}{3cm}
  \begin{verbatim}

L1: code(e)
    split L1 L2
L2: 
  \end{verbatim}
  \end{minipage}
  & Note the way that $e+$ needs one less split instruction than $e*$. \\ \hline
$e*?$ &
  \begin{minipage}{3cm}
  \begin{verbatim}

split L2 L1
L1: code(e)
    split L2 L1
L2: 
  \end{verbatim}
  \end{minipage}
  & This is just like the result of \verb'code(e*)' except that the
    labels are flipped. \\ \hline
$e+?$ &
  \begin{minipage}{3cm}
  \begin{verbatim}

L1: code(e)
    split L2 L1
L2: 
  \end{verbatim}
  \end{minipage}
  & This is just like the result of \verb'code(e+)' except that the
    labels are flipped. \\ \hline
$.$ & \verb'char SIGMA' & Here \verb'SIGMA' is $\Sigma$. \\ \hline
\end{tabular}
\end{table}

\section{Core Optimizations}
\label{section:skipscanopt}

This section presents the key contribution of this thesis. If any of
these optimization patterns can apply, they are applied first, then
the remainder of the regex is compiled using the ordinary compilation
path. In our actual implementation, optimizations are interleaved with
the standard compiler code paths, but it is easier to think about
them in a layered fasion. The applicability criterion for each
optimization is discussed below, and the relative precedence of different
optimizations is outlined in section \ref{section:optprecedence}.

This section just covers the most common cases; a discussion of
how these common cases are extended to deal with minor semantic
variations can be found in section \ref{section:optextend}.

\subsection{Greedy Dotstar Scan}

The expression $.*l$ matches anything up to and including the first
appearance of $l$. The standard code emitted for this pattern involves
spawning two new threads, $t_1$ and $t_2$, for every character of input.
Thread $t_1$ tries to start matching the literal terminator, while thread $t_2$
accepts any and prepares to spawn another pair of threads.
Thread $t_1$ will keep dying until the first occurrence of $l$,
at which point it will try to match the rest of the regex.
Thread $t_2$ will keep rolling forward, splitting off threads that try
to match $l$, but if $t_1$ or any of its children match
$t_2$ will never execute due to its lower precedence.

Now consider an alternate compilation strategy, where $.*l$ is compiled
to:

\begin{verbatim}
L1: scan-end l
    split L1 L2
L2: 
\end{verbatim}

The above code will scan to the first occurrence of $l$, and then produce
one thread trying to match the rest of the expression and another that
keeps scanning forward for new occurrences of $l$. We claim that this
code behaves just as the standard approach does. The only way to
break out of either loop is to find an instance of $l$ in the input,
and in both cases the match is continued in just the same way. Thus,
while saving significant thread spawning overhead and allowing the use
of a very fast string searching algorithm such as Tuned Boyer-Moore, this
optimization will not change the results of the regex execution
(\cite{Hume1991}).

It might seem like we would be justified in applying a similar 
optimization to \verb'e*l', but this would allow matches to occur
when they ought not to. \verb'scan-end' will scan forward over anything
but $l$, but it would only be legitimate to scan forward over repetitions
of $e$, something which can only be determined with full execution of
the regular expression.

\subsection{Trailing $.*$}

If $.*$ or a similar expression appears at the end of a regular expression,
it is asking to scan forward to the end of the input rather than scan
forward until the next expression matches. Then any trailing $.*$ can
be safely compiled to:

\begin{verbatim}
goto-end
\end{verbatim}

To determine if an expression is in a trailing position we can 
invoke algorithm \ref{algo:marktrailing} on the top level expression
to recursively mark expressions as trailing. This annotation pass
informs us about when it is valid to apply the trailing $.*$ optimization.

\begin{algorithm}
\caption{Mark Trailing Expressions} \label{algo:marktrailing}
\begin{algorithmic}
\Procedure{MarkTrailing}{$expr$}
  \State mark $expr$ as trailing
  \Switch{$expr$}
    \Case{$e_1 e_2$}
      \State MarkTrailing($e_2$)
      \State \textbf{break}
    \EndCase
    \Case{$e_1 \rvert e_2$}
      \State MarkTrailing($e_1$)
      \State MarkTrailing($e_2$)
      \State \textbf{break}
    \EndCase
    \Case{$e*$}
    \EndCase
    \Case{$e*?$}
    \EndCase
    \Case{$e+$}
    \EndCase
    \Case{$e+?$}
    \EndCase
    \Case{$(e)$}
      \State MarkTrailing($e$)
      \State \textbf{break}
    \EndCase
    \Case{$\epsilon$}
    \EndCase
    \Case{$\alpha$}
    \EndCase
    \Case{$.$}
    \EndCase
  \EndSwitch

  \Comment{Note that while MarkTrailing($e_1 e_2$) marks the outer
            concatenation expression as trailing, the inner expressions
            are marked separately so that $e_1$ is not marked as trailing
            and $e_2$ is.}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Literal-Terminated Scan}
\label{section:littermscan}

If an expression containing no capture groups is followed by a
literal or set of literals, it might be possible to just scan forward
to find that literal. It is possible to scan forward to find a literal
terminator with Tuned Boyer-Moore (\cite{Hume1991}) or a hardware
accelerated approach (\cite{GallantRegex}) and a group of literal
terminators using Aho-Corasick (\cite{Aho1975}) or Teddy (\cite{Teddy}).
For simplicity we just discuss scan optimization for expressions of
the form $el$, but it can just as easily apply to expressions of the
form $e(?:l1|l2|...|)$ or $e(l1|l2|...|)$.
If $L(.*l.*) \cap L(e) = \emptyset$ and $e$ does
not contain captures, then $l \not\in L(e)$\footnote{The details of deciding
this problem are laid out in section \ref{section:regexinterdecide}.}.
Thus we know that the first occurrence of $l$
in the input will not be part of the input which matches $e$. That means
it is legitimate to compile $el$ to:

\begin{verbatim}
scan-end l
\end{verbatim}

The idea of this optimization is straightforward, but ensuring that
it is applied to maximum effect requires a little more thought. There
are two main concerns. First, if two or more literals appear in an
expression we want to scan to the last one if possible. It wouldn't
do to process the expression from left to right because then we could
end up scanning forward to each literal when some could be skipped over
entirely. Second, if a literal appears in a concatenation of expressions
we want to make sure that the largest possible group of expressions
is skipped over. To accomplish these goals we can replace the approach
to compiling a concatenation of expressions given by the definition of
\verb'code' with algorithm \ref{algo:optconcat}.

\begin{algorithm}
\caption{Optimizing Concatenation} \label{algo:optconcat}
\begin{algorithmic}
\Procedure{CompileConcatOpt}{$e_1 ... e_n$}
  \For{$i \in n..i$}
    \If{$e_i$ is a literal we can scan to}
      \For{$j \in 0..i$}
        \If{$e_j ... e_{i-1} \cap .*i_i.* = \emptyset$}
          \Comment{This means we can scan to $e_i$}
          \State emit CompileConcatOpt($e_1 ... e_{j-1}$)
          \State emit \verb'scan' $e_i$
          \State emit code($e_{i+1} ... e_n$)
          \State \textbf{break}
        \EndIf
      \EndFor
      \State \textbf{break}
    \EndIf
  \EndFor
  \If{there were no literals in $e_1 ... e_n$}
    \State emit code($e_1 ... e_n$)
      \Comment{Give up on the optimization.}
  \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Note that in reality, the logic for the $el$ optimization must be
interleaved with the logic for the $.*l$ optimization, so the
real concatenation compiler routine is more complicated. For
expository purposes we consider them separately.

\subsection{Skip}
\label{section:skipopt}

The idea of the skip optimization is to compile a literal, $l$, to

\begin{verbatim}
skip len(l)
\end{verbatim}

The trouble is that just skipping over literals won't work at choice points.
To illustrate the problem, consider the regex \verb'/ac(.).|bc.(.)/'
on the input \verb'bcxy', which demonstrates a few different problems with
just compiling literals to skips.

% The first issue is that we will miss situations where literals are
% concatenated with other expressions with a statically known length.
% In this case $bc$ is concatenated with \verb|'.'|. To address this
% we implement logic which will attempt to compute the
% length of regular expressions. Any regular expression with a
% known length is then compiled into a skip.

The second issue is that there is no way to make sure that the
thread from a non-matching branch dies if the input fails to match in
the middle of a skip. By assumption, we know that the overall regular
expression matches, but we have no guarantee that any particular branch
matches. In the case of \verb'/ac(.).|bc.(.)/' on \verb'bcxy' the
first capture group parsed would be \verb'x' rather than \verb'y' as
it should be. The first thread has higher precedence and would
fail to die because the skip avoids looking at the \verb'a' char.
To resolve this issue we need a special compilation path for skips
that begin on the branches of a choice point. Examples of
choice points are the alternative operator ($\rvert$),
and the repetition operators ($*$, $+$, $*?$, $+?$).

Rather than compile to just a skip instruction, branches
at a choice operator have to compile to a char instruction
followed by a skip instruction.

\begin{verbatim}
char FIRST_SET
skip n-1
\end{verbatim}

Here \verb'FIRST_SET' is the set of characters which might
begin the regular expression ($fset(e_{branch})$).
Even this guard is not valid if the first sets of any two
branches at the branch point intersect. For example it would
not help deal with $a(.).|a.(.)$. In such cases we give up
on the optimization entirely.

\section{Optimization Variants and Extensions}
\label{section:optextend}

Section \ref{section:skipscanopt} outlines the most basic and most
important optimizations based on the \verb'skip' and \verb'scan-end'
instructions. There is, however, more to the story.
Real regular expression libraries provide several different repetition
operators, and the best way to leverage scan instructions to optimize
repetitions must take into account considerations around capture groups and
precedence.

% TODO: skip fusion
%While skipping over raw literals, and making sure to avoid
%falling prey to subtle ambiguities is important, we can do better.
%Both \verb'xxx' and \verb'xxx|yyy' ought to compile down to
%the same \verb'skip 3' instruction.

\subsection{Scan Variants}

\subsubsection{Lazy Dotstar Scan} %: {\tt code}($.*?l$)}

$.*?l$ gets compiled to:

\begin{verbatim}
L1: scan-end l
    split L2 L1
L2: 
\end{verbatim}

This optimization follows similar logic to that used for the greedy
dotstar optimization. At first it might appear that we would be able
to dispense with the non-determinism for the lazy variant, as laziness
means that we are after the shortest possible match so we don't
need to keep looking for future terminators. Unfortunately, there
might be a match failure in the expression which follows the
\verb'.*?'. More concretely, consider the regular expression
\verb'/.*?term(?:x)(.)/'
to be executed on \verb'aaaatermbbbbbbtermxy'. The correct solution will
capture the \verb'y' character, but without a non-deterministic branch
after the scan the scanning thread will just find the first terminator and
die. Thus, we still need non-determinism even in the lazy case.

\subsubsection{Capture Terminated Greedy Dotstar Scan}
  \label{capterm}

If there is a capture group around the literal we are scanning forward
for, we need to grab that capture. $.*(l)$ gets compiled to:

\begin{verbatim}
L1: scan-start l
    save n
    skip len(l)
    save n+1
    split L1 L2
L2: 
\end{verbatim}

\subsubsection{Capture Terminated Lazy Dotstar Scan}

If the repetition was lazy the operands to split are flipped.
$.*?(l)$ get compiled to:

\begin{verbatim}
L1: scan-start l
    save n
    skip len(l)
    save n+1
    split L2 L1
L2: 
\end{verbatim}

\subsubsection{Dotplus Scan}

All the optimizations for dotstar scanning also apply to dotplus
patterns (i.e. $.+l$). One might be concerned that the literal scan
might accidentally find its literal right at the start of the input,
something which should not be allowed because $.+$ demands at least
one character of input be consumed before the match can proceed.
Fortunately, the partial parsing assumption saves us. The input must
be in the language of the regex we are parsing, so there will always
be at least one leading character to scan over.

\subsection{Skip Extensions}

\subsubsection{Skip Choice: {\tt code}($e1 \rvert \dots \rvert en$)}

Section \ref{section:skipopt} already introduced the idea that
compiling skips at choice points requires a bit of extra thought.
This section goes into a more detail about how exactly skips are
handled in the context of an alternative.
This optimization applies iff none of the first sets of $e1 \dots en$
intersect. If they do intersect, we have to fall back on the standard
compilation strategy. Note that we can't just define this operation as
an optimization over a binary operator because we need a view of each
of the branches.

\begin{verbatim}
split L1 L2
L1: branch_code(e1)
    jmp LEND
L2: split L2ENTER L3
L2ENTER: branch_code(e2)
         jmp LEND
L3: split L3ENTER L4
L3ENTER: branch_code(e3)
         jmp LEND
...
LN: branch_code(en)
LEND:
\end{verbatim}

Here \verb'branch_code' behaves just like \verb'code' except 
for the differences outlined in table \ref{table:branchcode}.
Unless otherwise noted where \verb'code' recursively calls
itself, \verb'branch_code' recursively calls \emph{itself}
(the one exception is that tail of a concatenation is
compiled with the standard \verb'code' function).
The \verb'branc_code' function boils down to compiling literals
as a \verb'char' followed by a \verb'skip' rather than just
a \verb'skip', and making sure to do so recursively through
the structure of the expression.

\begin{table}
\caption{Differences Between Branch Code and Code}
This represents a modification to table \ref{table:codedef}.
\label{table:branchcode}
\centering
\begin{tabular}{| l | l |} \hline
Input Expression & Emitted VM Code \\ \hline
$l e$ &
  \begin{minipage}{4cm}
  \begin{verbatim}

char fset(l)
skip (len(l) - 1)
code(e)
  \end{verbatim}
  \end{minipage}
  \\ \hline
$l$ &
  \begin{minipage}{4cm}
  \begin{verbatim}

char fset(l)
skip (len(l) - 1)
  \end{verbatim}
  \end{minipage}
  \\ \hline
$e_1 e_2$ &
  \begin{minipage}{4cm}
  \begin{verbatim}

branch_code(e1)
code(e2)
  \end{verbatim}
  \end{minipage}
  \\ \hline
\end{tabular}
\end{table}
 
\subsubsection{Greedy Skip Star}

If the first sets of $e1$ and $e2$ don't intersect, $e1*e2$
is compiled as:

\begin{verbatim}
split L1 L2
L1: branch_code(e1)
    split L1 L2
L2: branch_code(e2)
\end{verbatim}

Where \verb'branch_code' is defined by table \ref{table:branchcode}.
The lazy variant just flips the split operands. If the first
sets to intersect, unoptimized code must be generated. The story
is really a little more complex because we have to consider
expressions of the form $e1*e2*...en$. In such cases, we much
check to see if any of the first sets of $e1$, $e2$, and $en$.

\subsubsection{Greedy Skip Plus}

If the first sets of $e1$ and $e2$ don't intersect.

\begin{verbatim}
L1: branch_code(e1)
    split L1 L2
L2: branch_code(e2)
\end{verbatim}

Where \verb'branch_code' is defined by table \ref{table:branchcode}.
The lazy variant just flips the split operands. Multiple
repeated expressions are handled just as they are for
Kleene star.

\section{Optimization Precedence}
\label{section:optprecedence}

When an optimization can be applied, it is, but when multiple
mutually exclusive optimizations can be applied, one must be
selected. While we have not defined a rigorous cost model
to do optimization selection, we do have some heuristics.
Our goal is to skip or scan over as much input as possible.
While skipping has the most potential for speedup, we
believe that scans are more likely to quickly consume large
chunks of input. Most literals are relatively small, while
it is quite a bit more common to see \verb'.*' used to eat
up large chunks of input. The $el$ optimization has the
potential to scan over the most preceding subexpressions\footnote{
Keep in mind that $e$ might be a concatenation.},
so we give it the highest precedence. The trailing \verb'.*' optimization
can never conflict with another optimization, so we don't have to
assign it a precedence. All that remains is the order of the \verb'.*l'
optimization and the literal skip optimization. If we gave the
literal skip optimization a higher precedence, the \verb'.*l'
optimization would never trigger. Additionally, the \verb'.*l'
optimization includes skipping over the literal terminator.
Then there is no reason to place the literal skip optimization
at a higher precedence. Table \ref{table:optprec} defines these
precedence levels concisely.

\begin{table}
\caption{Optimization Precedence}
\label{table:optprec}

\centering

\begin{tabular}{| c | c |} \hline
Optimization & Precedence Level (lower applies first) \\ \hline
\verb'el' & 1 \\ \hline
\verb'.*l' & 2 \\ \hline
Literal Scan & 3 \\ \hline
Trailing \verb'.*' & n/a \\ \hline
\end{tabular}
\end{table}

% TODO: why?
% \subsubsection{Fixed Length Skip: {\tt code}($s$)}
% 
% It is important that this optimization have lower precedence
% than the other skip optimizations. Here \verb's' is an
% expression with a statically known length.
% 
% \begin{verbatim}
% scan len(s)
% \end{verbatim}
