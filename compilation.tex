\chapter{Compilation}
\label{chapter:compilation}

\section{Notation}

\subsection{Regular Expressions}

Consider regular expressions described by the following AST. A
regular expression can decide whether or not a given string
matches and which substrings of the input match particular
subsections of the regular expression, called capture groups.
Here $\Sigma$ represents the alphabet from which characters in
the input string are drawn.

\begin{grammar}
  <e> ::= $e e$
  \alt $e \rvert e$
  \alt $(e)$
  \alt $\epsilon$
  \alt $\alpha$ {\tt where} $\alpha \subset \Sigma$
  \alt $e*$
  \alt $e+$
  \alt $e*?$
  \alt $e+?$
  \alt $.$
\end{grammar}

Here $e e$ represents concatenation of the two expressions,
$e|e$ represents choice between the two expressions, $(e)$ indicates a
capture group around $e$, $e*$ represents the Kleene closure of $e$,
$\epsilon$ represents the empty regular expression which matches nothing,
and $a$ indicates a regular expression which matches a character set.
These forms alone are enough to talk about regular expressions, as
the remaining forms can all be implemented in terms of this core set.
The simpler formulation is usually preferred for theoretical work because
it provides a smaller surface area that proofs must work with. 

Of the remaining forms, $e+$ represents $e$ repeated 1 or more times,
$e*?$, called lazy Kleene star, is a version of $e*$ which always prefers
the shortest match rather than the longest, $e+?$ is a version of
$e+$ which prefers the shortest match, and $.$ is a convenient
way to say $a$ where $a = \Sigma$. These more complex forms are
included despite the theoretical preference for simplicity in
order to facilitate discussion of optimizations.

Some of the more complex forms found in the source language of real
world regular expressions can be implemented more efficiently directly
than they would otherwise be if first implemented in terms of the
simpler form. For example, $e*$ requires an extra branch when compared
with $e+$ to account for the possibility of zero repetitions.
In order to fully explain the details of each of the
optimizations presented below, we therefore retain some of the more
interesting higher level forms. The implementation of greedy vs lazy
repetition ($e*$ vs $e*?$) differs
slightly, so we also include the extra repetition forms in order to
address those differences below.

Typical abstract regular expression syntax lacks a way to talk about
precedence separate from capture groups. For the sake of simplicity,
we have not added such a form to the above grammar, but it remains
useful to be able to write example regular expressions with explicit
precedence. In such cases we will use $(?:e)$ to indicate a grouping
of $e$ without a corresponding capture. This is the same concrete syntax
provided by the rust regex crate.

\subsection{Literals}

Literals are fixed strings of characters of length one or more.
Stated differently, $l$ is called a literal if $l = a+$ for $a \in \Sigma$.

\section{First Sets}

In order to make sure that the correct branch is taken in cases of
ambiguity, we will need to be able to talk about the set of characters
which might possibly start a regex match. To this end we introduce the
notion of a first set here. Let $e$ be a regular expression.
${first(w) \rvert w \in L(e)}$ where $first$ takes just the first charicter
from a word, is the first set for $e$. The first set for a given
regular expression can be computed by the recursive function $fset$,
which is defined as follows

\[
  fset(e_1 e_2) = fset(e_1)
\]
\[
  fset(e_1 \rvert e_2) = fset(e_1) \cup fset(e_2)
\]
\[
  fset((e)) = fset(e)
\]
\[
  fset(\epsilon) = \emptyset
\]
\[
  fset(\alpha) = \alpha
\]
\[
  fset(e*) = fset(e+) = fset(e*?) = fset(e+?) = fset(e)
\]
\[
  fset(.) = \Sigma
\]


\section{The Regular Expression VM}

One approach to regular expression matching is to compile regular expressions
to a set of bytecode instructions for a specially designed VM. There
are several different potential VM implementations, each with its own
set of trade offs, but each of these back-ends share the same bytecode
and compiler front end. In order to understand the compiler, it
is important to first develop an understanding of the abstractions
that the target machine provides.

Regex VM instructions are executed in the context of a VM thread.
A VM thread consists of an instruction pointer, which indicates
the next VM instruction to be executed, an input cursor which
indicates the current position in the input to be matched, and
a finite number of capture slots. Capture slots are uses to
track the start and end of capture groups with one slot for each
end of the capture. After the regex execution, the capture groups
are reconstructed from these slots. The \verb'save' instruction is
responsible for placing the appropriate breadcrumbs in the slots.
Threads may spawn multiple child threads at once which are logically
non-deterministically executed. This first class non-determinism 
makes it possible to easily encode the natural non-determinism of
a regular expression. The main project of regex VM back-ends is
efficiently managing the non-determinism.

VM instructions can be thought of as belonging to three different categories:
control flow, tests, and bookkeeping. Control flow instructions deal with
spawning, killing, and modifying threads. Tests examine the input to determine
whether the current thread matches. Bookkeeping instructions track metadata,
in particular information about where capture groups begin and end.
Many of the instructions come from standard regex simulation
techniques, but several have been added to support the optimizations
presented here. Complete descriptions of the instructions can be found
in table \ref{tab:insts}.

\subsection{Code Blocks and Addresses}

Regex VM instructions are laid out in a linear fashion, just like
machine code for real processors. A VM program consists of a list of
VM bytecodes, each of which is addressed by its offset. During
execution, an instruction pointer indicates the current instruction.
After an instruction has been executed, the instruction pointer
is incremented unless the definition of the instruction (such as
the \verb'jmp' instruction) says otherwise. Labels can be used
in place of fixed addresses to provide convenient names for
use in branching code. A label is equivalent to the address
of the next instruction which appears.

\begin{table}[ht]

\centering
\vspace*{-2.5cm}
\begin{adjustwidth}{-2cm}{}

\begin{tabular}{ | l | c | c | p{10cm} | } \hline
Instruction & Inst Type(s) & Extension & Description \\ \hline
{\tt split $l_1$ $l_2$} & Control Flow & No &
    Tells the VM to move the instruction
    pointer to $l_1$, and spawn another thread at
    label $l_2$. If the parent thread or any of its
    children match and the child thread or any of its children match,
    choose the match from the parent thread tree. Thus the code

        \begin{verbatim}
        split L1 L2
        L1: save 2
            char {x|y}
            save 3
        L2: save 4
            char {y|z}
            save 5
        match
        \end{verbatim}

    executed on \verb'y' would result in the first capture group
    (specified by slots 2 and 3) being populated with a \verb'y'
    even though both branches would result in a match. On the
    other hand, if the labels in the split instruction were
    flipped, the second capture slot (specified by slots 4 and 5)
    would be populated with \verb'y' instead.
    \\ \hline
{\tt jmp $l_1$} & Control Flow & No &
    Tells the VM to set the instruction pointer
    to $l_1$. Note that the input cursor does not move.  \\ \hline
{\tt match} & Control Flow & No &
    Indicates a match. The VM may stop. \\ \hline
{\tt skip $n$} & Control Flow & Yes &
    Tells the VM to advance the input
    cursor by $n$. Skip is an extension instruction provided specifically
    to facilitate the optimizations presented here. \\ \hline
{\tt save $s$} & Bookkeeping & No &
    Tells the VM to save the current string
    offset into capture slot $s$. \\ \hline
{\tt char $\alpha$} & Test & No &
    Tells the VM to kill the current thread unless
    the current char is in $\alpha$ (where $\alpha \subset \Sigma$).
    If the current char is in $\alpha$, the VM must advance the input cursor
    for the current thread. \\ \hline
{\tt scan-end $l$} & Test \& Control Flow & Yes &
    This test instruction tells the VM to scan forward in the input to find
    the literal $l$. If $l$ is not found, the VM must kill the current thread,
    otherwise it must place the input cursor
    right after the end of $l$ in the input. For example executing

        \begin{verbatim}
        scan-end ``foo''
        \end{verbatim}

    on \verb'xxxxfooyy' would place the string pointer at the first \verb'y'
    char. \verb'scan-end' is an extension instruction.

    While a test instruction, the \verb'scan-end' instruction can
    significantly advance the input cursor, so it can also be classified
    as a control flow instruction. \\ \hline
{\tt scan-begin$l$} & Test \& Control Flow & Yes &
    This instruction behaves just like \verb'scan-end' except that the
    VM must place the input cursor at the beginning of the literal on
    a successful match. \verb'scan-begin' is an extension instruction.
    \\ \hline
{\tt goto-end $l$} & Control Flow & Yes &
    This instruction tells the VM to just jump to the end of the input.
    It is included to facilitate optimizing trailing $.*$ expressions.
    \\ \hline
\end{tabular}
\end{adjustwidth}

\caption{VM Instructions}
\label{tab:insts}
\end{table}

\section{Compilation}

In this section, we define \verb'code', a compilation function mapping
from regular expressions to VM instructions. Compilation of regular
expressions for an ordinary VM proceeds as follows. In the context
of VM instructions, sets are written in \verb'SCREAMING_SNAKE_CASE'.

\subsection{{\tt code}($e1 e2$)}

\begin{verbatim}
code(e1)
code(e2)
\end{verbatim}

\subsection{{\tt code}($e1 \rvert e2$)}

\begin{verbatim}
split L1 L2
L1: code(e1)
    jmp L3
L2: code(e2)
L3:
\end{verbatim}

\subsection{{\tt code}($(e)$)}

\begin{verbatim}
save n
code(e)
save n+1
\end{verbatim}

Here \verb'n' is given by $(i*2) + 2$ where $i$ is the index of
the current capture group in the regular expression. Slots $0$
and $1$ are reserved for capturing the entire match, which is
where the extra $2$ in the formula comes from. A nice corollary
is that we can always know the number of slots that
will be required ahead of time by computing $(c*2) + 2$
where $c$ is the total number of capture groups.

\subsection{{\tt code}($\epsilon$)}

The empty code block.

\subsection{{\tt code}($a$)}

\begin{verbatim}
char a
\end{verbatim}

\subsection{{\tt code}($e*$)}

\begin{verbatim}
split L1 L2
L1: code(e)
    split L1 L2
L2: 
\end{verbatim}

\subsection{{\tt code}($e+$)}

\begin{verbatim}
L1: code(e)
    split L1 L2
L2: 
\end{verbatim}

\subsection{{\tt code}($e*?$)}

\begin{verbatim}
split L2 L1
L1: code(e)
    split L2 L1
L2: 
\end{verbatim}

\subsection{{\tt code}($e+?$)}

\begin{verbatim}
L1: code(e)
    split L2 L1
L2: 
\end{verbatim}

\subsection{{\tt code}($.$)}

\begin{verbatim}
char SIGMA
\end{verbatim}

\section{Regular Expression Intersection: A Decision Problem}

In this section we introduce the concept of regular expression
intersection, and specialize it to a decision problem determining
if the intersection of two regex is empty. This is a useful
piece of analysis for performing optimizations on skip regex
because an empty intersection indicates that no string is in
the language of both regex. This means that in a context where
a particular chunk of input might correspond to one of several
non-intersecting sub-expressions, a match against one sub-expression
immediately eliminates the need to check the other sub-expressions.

\subsection{Intersection Automata}

The easiest way to reason about regular expression intersection
is through automita. To decide the intersection of two NFAs we
can just run them both in lockstep on the same input stream
and see if they both end up in accepting states. Running two
NFAs in lockstep requires a specific input, so it does not allow
us to do any static checking, but it sets us in the right direction.
A regex automita decides the empty set if no final state is reachable
from the start state, so if we can construct a new NFA which simulates
running two NFAs in lockstep, we will be able to statically determine
if the intersection of two regex is empty. We construct an
intersecting automita by taking the Cartesian product
of the two state sets to form a new state set, then adding edges
between states only where both NFAs could make progress on a particular
character of input. This idea is unpacked more formally below.

Let $e_1$ and $e_2$ be regular expressions. Let
$n_1 = \langle Q_1, \Sigma, \Delta_1, q_{\{1,0\}}, F_1 \rangle$ and
$n_2 = \langle Q_2, \Sigma, \Delta_2, q_{\{2,0\}}, F_2 \rangle$
be NFAs such that $n_1$ decides $e_1$ and $n_2$ decides $e_2$.
Construct $n_i = \langle Q_i, \Sigma, \Delta_i, \langle q_{\{1,0\}}, q_{\{2,0\}} \rangle, F_i \rangle$
where
$$Q_i = Q_1 \times Q_2$$
$$F_i = \{ \langle q_1, q_2 \rangle \mid q_1 \in F_1 \wedge q_2 \in F_2\}$$
\begin{equation}
\begin{split}
  \Delta_i = \{ \langle \langle q_{\{1, f\}}, q_{\{2, f\}} \rangle, c \rangle
                    \rightarrow \langle q_{\{1, t\}}, q_{\{2, t\}} \rangle
                \mid
                  (\langle q_{\{1, f\}}, c \rangle \rightarrow q_{\{1, t\}}) \in \Delta_1
                  \wedge
                  (\langle q_{\{2, f\}}, c \rangle \rightarrow q_{\{2, t\}}) \in \Delta_2
             \} \\
             \cup
             \{
                \langle \langle q_{\{1, f\}}, q_{\{2, f\}} \rangle, \epsilon \rangle
                  \rightarrow \langle q_{\{1, f\}}, q_{\{2, t\}} \rangle
                \mid
                  (\langle q_{\{1, f\}}, c \rangle \rightarrow q_{\{1, t\}}) \in \Delta_1
                  \wedge
                  (\langle q_{\{2, f\}}, \epsilon \rangle \rightarrow q_{\{2, t\}}) \in \Delta_2
             \} \\
             \cup
             \{
                \langle \langle q_{\{1, f\}}, q_{\{2, f\}} \rangle, \epsilon \rangle
                  \rightarrow \langle q_{\{1, t\}}, q_{\{2, f\}} \rangle
                \mid
                  (\langle q_{\{1, f\}}, \epsilon \rangle \rightarrow q_{\{1, t\}}) \in \Delta_1
                  \wedge
                  (\langle q_{\{2, f\}}, c \rangle \rightarrow q_{\{2, t\}}) \in \Delta_2
             \} \\
             \cup
             \{
                \langle \langle q_{\{1, f\}}, q_{\{2, f\}} \rangle, \epsilon \rangle
                  \rightarrow \langle q_{\{1, t\}}, q_{\{2, t\}} \rangle
                \mid
                  (\langle q_{\{1, f\}}, \epsilon \rangle \rightarrow q_{\{1, t\}}) \in \Delta_1
                  \wedge
                  (\langle q_{\{2, f\}}, \epsilon \rangle \rightarrow q_{\{2, t\}}) \in \Delta_2
             \}
\end{split}
\end{equation}

The most interesting part of this construction is the creation of the
new delta function. Edges are only added to $\Delta_i$ when a particular
character $c$ shows up in both input delta functions. Epsilon edges count
as wild card characters.

To illustrate this process consider the intersection of NFAs corresponding
to \verb'/aa/' (figure \ref{fig:NFAaa}) and \verb'/ab/'
(figure \ref{fig:NFAab}), shown in figure \ref{fig:NFAaa:ab}. Note that
we have omitted intermediary states with no edges for brevity, but the
full intersection NFA would have states $q_{0,1}, q_{1,2}$ and so on.
It is easy to see by inspection of the diagram that the intersection of
\verb'/aa/' and \verb'/ab/' is empty. This is to be expected because
\verb'/aa/' does not accept any strings containing \verb'b', while
\verb'/b/' demands a \verb'b' in its input.

To see an example where the intersection is non-empty, consider the
intersection of \verb'/aa/' (figure \ref{fig:NFAaa}) and \verb'/a*/'
(figure \ref{fig:NFAastar}), shown in figure \ref{fig:NFAaa:astar}.
Again, we have omitted the orphan intermediary states for brevity.
The path $q_{0,0}, q_{0,1}, q_{1,2}, q_{1,1}, q_{2,3}$, flows from
the start state to the accepting state, so the intersection of the
two regex is not empty, as we would expect.

\begin{figure}
\centering
\caption{{\tt /aa/} NFA}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance = 2cm,semithick]
  \tikzstyle{every state}=[text=black]

  \node[initial,state]   (Q0)                {$q_0$}; 
  \node[state]           (Q1) [right of=Q0]  {$q_1$}; 
  \node[state,accepting] (Q2) [right of=Q1]  {$q_2$}; 
  \path (Q0) edge node {$a$} (Q1)

        (Q1) edge node {$a$} (Q2);
\end{tikzpicture}
\label{fig:NFAaa}
\end{figure}

\begin{figure}
\centering
\caption{{\tt /ab/} NFA}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance = 2cm,semithick]
  \tikzstyle{every state}=[text=black]

  \node[initial,state]   (Q0)                {$q_0$}; 
  \node[state]           (Q1) [right of=Q0]  {$q_1$}; 
  \node[state,accepting] (Q2) [right of=Q1]  {$q_2$}; 
  \path (Q0) edge node {$a$} (Q1)

        (Q1) edge node {$b$} (Q2);
\end{tikzpicture}
\label{fig:NFAab}
\end{figure}

\begin{figure}
\centering
\caption{{\tt /aa/} $\cap$ {\tt /ab/} NFA}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance = 2cm,semithick]
  \tikzstyle{every state}=[text=black]

  \node[initial,state]   (Q00)                 {$q_{0,0}$}; 
  \node[state]           (Q11) [right of=Q00]  {$q_{1,1}$}; 
  \node[state,accepting] (Q22) [right of=Q11]  {$q_{2,2}$}; 
  \path (Q00) edge node {$a$} (Q11)
        ;
\end{tikzpicture}
\label{fig:NFAaa:ab}
\end{figure}

\begin{figure}
\centering
\caption{{\tt /a*/} NFA}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance = 2cm,semithick]
  \tikzstyle{every state}=[text=black]

  \node[initial,state]   (Q0)                {$q_0$}; 
  \node[state]           (Q1) [right of=Q0]  {$q_1$}; 
  \node[state]           (Q2) [right of=Q1]  {$q_2$}; 
  \node[state,accepting] (Q3) [right of=Q2]  {$q_3$}; 
  \path (Q0) edge node {$\epsilon$} (Q1)
             edge [bend right] node {$\epsilon$} (Q3)

        (Q1) edge node {$a$} (Q2)

        (Q2) edge node {$\epsilon$} (Q3)
             edge [bend right] node {$\epsilon$} (Q1)
        ;
\end{tikzpicture}
\label{fig:NFAastar}
\end{figure}

\begin{figure}
\centering
\caption{{\tt /aa/} $\cap$ {\tt /a*/} NFA}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance = 2cm,semithick]
  \tikzstyle{every state}=[text=black]

  \node[initial,state]   (Q00)                {$q_{0,0}$}; 
  \node[state]           (Q01) [below of=Q00] {$q_{0,1}$}; 
  \node[state]           (Q03) [below of=Q01] {$q_{0,3}$}; 

  \node[state]           (Q11) [right of=Q00] {$q_{1,1}$}; 
  \node[state]           (Q12) [below of=Q11] {$q_{1,2}$}; 
  \node[state]           (Q13) [below of=Q12] {$q_{1,3}$}; 

  \node[state]           (Q22) [right of=Q11] {$q_{2,2}$}; 
  \node[state]           (Q21) [below of=Q22] {$q_{2,1}$}; 
  \node[state,accepting] (Q23) [below of=Q22] {$q_{2,3}$}; 

  \path (Q00) edge [bend left]   node {$\epsilon$} (Q01)
        (Q00) edge [bend right]  node {$\epsilon$} (Q03)

        (Q01) edge node {$a$} (Q12)

        (Q12) edge node {$\epsilon$} (Q11)
        (Q12) edge node {$\epsilon$} (Q13)

        (Q11) edge node {$a$} (Q22)
        (Q11) edge node {$a$} (Q23)
        (Q11) edge node {$a$} (Q21)
        ;
\end{tikzpicture}
\label{fig:NFAaa:astar}
\end{figure}

% TODO: proof.

\subsection{Regex Intersection on VM Instructions}

While regular expression intersection is most elegantly stated
in terms of abstract NFAs and the construction of an entirely
new automata, this approach needs to be modified if it is
to be used in the context of a VM NFA simulation. The regex
VM instructions used for implementation are a representation
of an NFA, but not in the usual set-of-states and transition
table sort of way. Additionally, constructing an entirely
new NFA consumes space on the order of the product of the
size of the two input NFAs. Instead, we use the concept of
iterators, a key tool for the Rust programming language,
to provide the ability to explore a compound NFA without
ever materializing it. Rather than using a compiled
skip regex we use a regex compiled to use the unextended
set of VM instructions. This simplifies our job, and prevents
compiling skip instructions from recursively compiling any
subexpressions.

To see how to decide the intersection of two NFAs described
by VM instructions, consider the algorithm laid out in
algorithm \ref{algo:regexinter}. In the algorithm presented
we use iterator generators (expressed with the \verb'yield' keyword),
a feature common to programming languages which support coroutines
such as Python or Lua. Helper iterators over a single NFA are defined
in algorithm \ref{algo:nfaiter}. Rust does not yet support generators, so the
actual implementation instead uses explicit state machines, but
we use generators here to make it easier to focus on the underlying
algorithm.

\begin{algorithm}
\caption{VM NFA Intersection} \label{algo:regexinter}
\begin{algorithmic}
\Procedure{NFAIntersectionIsEmpty}{$n_1$, $n_2$}
  \State $resume \gets [\langle start(n_1), start(n_2) \rangle]$
  \State $seen \gets \{\}$
  \While{$\langle s_1, s_2 \rangle \gets resume.pop()$}
    \If{$\langle s_1, s_2 \rangle \in seen$}
      \State \textbf{continue}
    \EndIf
    \State $seen \gets seen \cup \{\langle s_1, s_2 \rangle\}$
    \If{$s_1 = {\tt match} \wedge s_2 = {\tt match}$}
      \State \textbf{return} true
    \EndIf
    \State $resume.extend(IntersectChildren(s_1, s_2))$
  \EndWhile
  \State \textbf{return} false
\EndProcedure
\Procedure{IntersectChildren}{$inst_1$, $inst_2$}
  \For{$c1 \in$ Children($inst_1$)}
    \For{$c2 \in$ Children($inst_2$)}
      \Switch{$c1$}
        \Case{{\tt match}}
          \State \textbf{yield} $\langle c1, c2 \rangle$
          \State \textbf{break}
        \EndCase
        \Case{{\tt char} $\alpha_1$}
          \Switch{$c2$}
            \Case{{\tt match}}
              \State \textbf{yield} $\langle c1, c2 \rangle$
              \State \textbf{break}
            \EndCase
            \Case{{\tt char} $\alpha_2$}
              \If{$\alpha_1 \cap \alpha_2 \not= \emptyset$}
                \State \textbf{yield} $\langle c1, c2 \rangle$
              \EndIf
              \State \textbf{break}
            \EndCase
            \Case{default}
              \State unreachable
            \EndCase
          \EndSwitch
          \State \textbf{break}
        \EndCase
        \Case{default}
          \State unreachable
        \EndCase
      \EndSwitch
    \EndFor
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{VM NFA Iteration} \label{algo:nfaiter}
\begin{algorithmic}
\Procedure{Children}{$inst$}
  \State clear seen markings
  \Switch{$inst$}
    \Case{{\tt split L1 L2}}
      \State Terminals({\tt L1})
      \State Terminals({\tt L2})
      \State \textbf{break}
    \EndCase
    \Case{{\tt jmp L}}
      \State Terminals({\tt L})
      \State \textbf{break}
    \EndCase
    \Case{{\tt match}} \EndCase
    \Case{{\tt char} $\alpha$} \EndCase
    \Case{{\tt save} $s$}
      \State Terminals($next(inst)$)
      \State \textbf{break}
    \EndCase
  \EndSwitch
\EndProcedure
\Procedure{Terminals}{$inst$}
  \If{$inst$ is marked as seen}
    \State \textbf{return}
  \EndIf
  \State mark $inst$ as seen
  \Switch{$inst$}
    \Case{{\tt split L1 L2}}
      \State Terminals({\tt L1})
      \State Terminals({\tt L2})
      \State \textbf{break}
    \EndCase
    \Case{{\tt jmp L}}
      \State Terminals({\tt L})
      \State \textbf{break}
    \EndCase
    \Case{{\tt match}} \EndCase
    \Case{{\tt char} $\alpha$}
      \State \textbf{yield} $inst$
      \State \textbf{break}
    \EndCase
    \Case{{\tt save} $s$}
      \State Terminals($next(inst)$)
      \State \textbf{break}
    \EndCase
  \EndSwitch
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Skip and Scan Optimizations} \label{skipscanopt}

Sometimes we can do better than the default compilation path. If any of
these optimization patterns can apply, they are applied first, then
the remainder of the regex is compiled using the ordinary compilation
path.

\subsection{Greedy Dotstar Scan: {\tt code}($.*l$)}

$.*l$ matches anything up to and including the first appearance of $l$.
The standard code emitted for this pattern involves spawning two
new threads for every character of input, one to start matching
the literal and one to loop back and try again on the next character
of input. In this case the matching thread will keep dying until
the first occurrence of $l$, at which point the matching thread will
try to match the rest of the regex, and the loop-back thread will
continue rolling forward splitting off threads trying to match $l$.

Now consider an alternate compilation strategy, where $.*l$ is compiled
to:

\begin{verbatim}
L1: scan-end l
    split L1 L2
L2: 
\end{verbatim}

This will first scan to the first occurrence of $l$, and then produce
one thread trying to match the rest of the expression and another that
keeps scanning forward for new occurrences of $l$. The only way to
break out of either loop is to find an instance of $l$ in the input,
and in both cases the match is continued in just the same way. Thus,
while saving significant thread spawning overhead and allowing the use
of a very fast string searching algorithm such as Boyer-Moore, this
optimization will not change the results of the regex execution.

It might seem like we would be justified in applying a similar 
optimization to \verb'e*l', but this would allow matches to occur
when they ought not to. \verb'scan-end' will scan forward over anything
but $l$, but it would only be legitimate to scan forward over repetitions
of $e$, something which can only be determined with full execution of
the regular expression.

\subsection{Trailing $.*$: {\tt code}($.*$) }

If $.*$ or a similar expression appears at the end of a regular expression,
it is asking to scan forward to the end of the input rather than scan
forward until the next expression matches. Then any trailing $.*$ can
be safely compiled to:

\begin{verbatim}
goto-end
\end{verbatim}

To determine if an expression is in a trailing position we can 
invoke algorithm \ref{algo:marktrailing} on the top level expression
to recursively mark expressions as trailing. This annotation pass
informs us about when it is valid to apply the trailing $.*$ optimization.

\begin{algorithm}
\caption{Mark Trailing Expressions} \label{algo:marktrailing}
\begin{algorithmic}
\Procedure{MarkTrailing}{$expr$}
  \State mark $expr$ as trailing
  \Switch{$expr$}
    \Case{$e_1 e_2$}
      \State MarkTrailing($e_2$)
      \State \textbf{break}
    \EndCase
    \Case{$e_1 \rvert e_2$}
      \State MarkTrailing($e_1$)
      \State MarkTrailing($e_2$)
      \State \textbf{break}
    \EndCase
    \Case{$e*$}
    \EndCase
    \Case{$e*?$}
    \EndCase
    \Case{$e+$}
    \EndCase
    \Case{$e+?$}
    \EndCase
    \Case{$(e)$}
      \State MarkTrailing($e$)
      \State \textbf{break}
    \EndCase
    \Case{$\epsilon$}
    \EndCase
    \Case{$\alpha$}
    \EndCase
    \Case{$.$}
    \EndCase
  \EndSwitch
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Literal Terminated Scan: {\tt code}($el$) }

If $L(.*l.*) \cap L(e) = \emptyset$ and $e$ does not contain captures,
then $l \not\in L(e)$. Thus we know that the first occurrence of $l$
in the input will not be part of the input which matches $e$. That means
it is legitimate to compile $el$ to:

\begin{verbatim}
scan-end l
\end{verbatim}

The idea of this optimization is straightforward, but ensuring that
it is applied to maximum effect requires a little more thought. There
are two main concerns. First, if two or more literals appear in an
expression we want to scan to the last one if possible. It wouldn't
do to process the expression from left to right because then we could
end up scanning forward to each literal when some could be skipped over
entirely. Second, if a literal appears in a concatenation of expressions
we want to make sure that the largest possible group of expressions
is skipped over. To accomplish these goals we can replace the approach
to compiling a concatenation of expressions given by the definition of
\verb'code' with algorithm \ref{algo:optconcat}.

\begin{algorithm}
\caption{Optimizing Concatenation} \label{algo:optconcat}
\begin{algorithmic}
\Procedure{CompileConcatOpt}{$e_1 ... e_n$}
  \For{$i \in n..i$}
    \If{$e_i$ is a literal we can scan to}
      \For{$j \in 0..i$}
        \If{$e_j ... e_{i-1} \cap .*i_i.* = \emptyset$}
          \Comment{This means we can scan to $e_i$}
          \State emit CompileConcatOpt($e1 ... e_{j-1}$)
          \State emit \verb'scan' $e_i$
          \State emit code($e_{i+1} ... e_n$)
          \State {\b Break}
        \EndIf
      \EndFor
      \State {\b Break}
    \EndIf
  \EndFor
  \If{There were no literals in $e_1 ... e_n$}
    emit code($e_1 ... e_n$)
  \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Skip}

The idea of the skip optimization is to compile a literal, $l$, to

\begin{verbatim}
skip len(l)
\end{verbatim}

The trouble is that this won't work at choice points. To illustrate the
problem, consider the regex $ac(.). \rvert bc.(.)$ on the input
\verb'bcxy'. This demonstrates a few different problems with just
compiling literals to skips.

The first issue is that we will miss situations where literals are
concatenated with other expressions with a statically known length.
In this case $bc$ is concatenated with $. \:$. To address this
we implement skip fusion logic which will attempt to compute the
length of regular expressions. Any regular expression with a
known length is then compiled into a skip.

The second issue is that there is no way to make sure that the
thread from a non-matching branch dies if the input fails to match in
the middle of a skip. By assumption, we know that the overall regular
expression matches, but we have no guarantee that any particular branch
matches. In the case of $ac(.). \rvert bc.(.)$ on \verb'bcxy' the
first capture group parsed would be \verb'x' rather than \verb'y' as
it should be. The first thread has higher precedence and would
fail to die because the skip avoids looking at the \verb'a' char.
To resolve this we need a special compilation path for skips
that begin on the branches of a choice point. Examples of
choice points are the alternative operator ($\rvert$),
and the repetition operators ($*$, $+$, $*?$, $+?$).

Rather than compile to just a skip instruction, branches
at a choice operator have to compile to a char instruction
followed by a skip instruction.

\begin{verbatim}
char FIRST_SET
skip n-1
\end{verbatim}

Here \verb'FIRST_SET' is the set of characters which might
begin the regular expression ($fset(e_{branch})$).

Even this guard is not valid if the first sets of any two
branches at the branch point intersect. For example it would
not help deal with $a(.).|a.(.)$. In such cases we have to fall
back to the default compilation model.

\section{Optimization Variants and Extensions}

Section \ref{skipscanopt} outlines the most basic and most
important optimizations based off of the \verb'skip' and \verb'scan-end'
instructions. There is, however, more to the story.
Real regular expression libraries provide several different repetition
operators, and the best way to leverage scan instructions to optimize
repetitions must take into account considerations around capture groups and
precedence.

While skipping over raw literals, and making sure to avoid
falling prey to subtle ambiguities is important, we can do better.
Both \verb'xxx' but not \verb'xxx|yyy' ought to compile down to
the same \verb'skip 3' instruction. This section will include 
further skip optimizations which spend more effort analyzing 
the regular expression to be skipped over to determine a
statically known length.

\subsection{Scan Variants}

\subsubsection{Lazy Dotstar Scan: {\tt code}($.*?l$)}

\begin{verbatim}
L1: scan-end l
    split L2 L1
L2: 
\end{verbatim}

This optimization follows similar logic to that used for the greedy
dotstar optimization. At first it might appear that we would be able
to dispense with the non-determinism for the lazy variant, as laziness
means that we are after the shortest possible match so we don't
need to keep looking for future terminators. Unfortunately, there
might be a match failure in the expression which follows the lazy
dotstar. More concretely, consider the regular expression $.*?term(?:x)(.)$
to be executed on \verb'aaaatermbbbbbbtermxy'. The correct solution will
capture the \verb'y' character, but without a non-deterministic branch
after the scan the scanning thread will just find the first terminator and
die. Thus, we still need non-determinism even in the lazy case.

\subsubsection{Capture Terminated Greedy Dotstar Scan: {\tt code}($.*(l)$)}
  \label{capterm}

If there is a capture group around the literal we are scanning forward
for, we need to grab that capture.

\begin{verbatim}
L1: scan-start l
    save n
    skip len(l)
    save n+1
    split L1 L2
L2: 
\end{verbatim}

\subsubsection{Capture Terminated Lazy Dotstar Scan: {\tt code}($.*?(l)$)}

\begin{verbatim}
L1: scan-start l
    save n
    skip len(l)
    save n+1
    split L2 L1
L2: 
\end{verbatim}

\subsubsection{[Capture Terminated] \{Lazy $\rvert$ Greedy\} Dotplus Scan}

All the optimizations for dotstar scanning also apply to dotplus
patterns ($.+l$).

\subsection{Skip Extension}

\subsubsection{Skip Choice: {\tt code}($e1 \rvert \dots \rvert en$)}

Applies iff none of the first sets of $e1 \dots en$ intersect. Note that
we can't just define this operation as an optimization over a binary
operator because we need a view of each of the branches.

\begin{verbatim}
split L1 L2
L1: branch_code(e1)
    jmp LEND
L2: split L3 L2ENTER
L2ENTER: branch_code(e2)
         jmp LEND
L3: split L4 L3ENTER
L3ENTER: branch_code(e3)
         jmp LEND
...
LN: branch_code(en)
LEND:
\end{verbatim}

Here branch code behaves just like \verb'code' except that \\
\verb'branch_code(le) = char fset(l); skip (len(l) - 1); code(e)', \\
\verb'branch_code(l) = fset(l); skip(len(l) - 1)', \\
\verb'branch_code(e1 e2) = branch_code(e1); code(e2)', \\
and \verb'branch_code' is used to recursively compile
other expressions rather than \verb'code'. This boils
down to compiling literals as a \verb'char' followed
by a \verb'skip' rather than just a \verb'skip', and
making sure to do so recursively through the structure
of the expression.

\subsubsection{Greedy Skip Star: {\tt code}($e1*e2$)}

If the first sets of $e1$ and $e2$ don't intersect.

\begin{verbatim}
split L1 L2
L1: branch_code(e1)
    split L1 L2
L2: branch_code(e2)
\end{verbatim}

Where \verb'branch_code' is defined in the same way as above.
The lazy variant just flips the skip operands.

\subsubsection{Greedy Skip Plus: {\tt code}($e1+e2$)}

If the first sets of $e1$ and $e2$ don't intersect.

\begin{verbatim}
L1: branch_code(e1)
    split L1 L2
L2: branch_code(e2)
\end{verbatim}

Where \verb'branch_code' is defined in the same way as above.
The lazy variant just flips the skip operands as usual.

\subsubsection{Literal Skip: {\tt code}($s$)}

It is important that this optimization have lower precedence
than the other skip optimizations. Here \verb's' is an
expression with a statically known length.

\begin{verbatim}
scan len(s)
\end{verbatim}
